{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/videollm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "import decord\n",
    "from decord import VideoReader\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def sentene2verb(sentence):\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb_phrase = token.lemma_\n",
    "            verbs.append(verb_phrase)\n",
    "    return verbs\n",
    "\n",
    "def sentene2n(sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            verb_phrase = token.lemma_\n",
    "            verbs.append(verb_phrase)\n",
    "    return verbs\n",
    "\n",
    "def ceil_time_by_fps(time: float, fps: int, min_time: float, max_time: float):\n",
    "    return min(max(math.ceil(time * fps) / fps, min_time), max_time)\n",
    "\n",
    "def show_image(load_range, frames, output_path=None):\n",
    "    frames_per_row = 7\n",
    "\n",
    "    # 计算行数\n",
    "    rows = math.ceil(len(load_range) / frames_per_row)\n",
    "\n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(rows, frames_per_row, figsize=(frames_per_row * 4, rows * 4))\n",
    "\n",
    "    # 将 frames 绘制到子图中\n",
    "    for i in range(len(load_range)):\n",
    "        row = i // frames_per_row\n",
    "        col = i % frames_per_row\n",
    "        if rows == 1:\n",
    "            axes[col].imshow(frames[i])\n",
    "            axes[col].axis('off')\n",
    "            axes[col].set_title(f\"Frame {i}\")\n",
    "        else:\n",
    "            axes[row, col].imshow(frames[i])\n",
    "            axes[row, col].axis('off')\n",
    "            axes[row, col].set_title(f\"Frame {i}\")\n",
    "\n",
    "    # 如果最后一行有空的子图格子，关闭它们\n",
    "    for i in range(len(load_range), rows * frames_per_row):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class AnnotationLoader:\n",
    "    def __init__(self, train_path, val_path, origin_path):\n",
    "        self.train_data = json.load(open(train_path))\n",
    "        self.val_data = json.load(open(val_path))\n",
    "        self.data = {**self.train_data, **self.val_data}\n",
    "        \n",
    "        self.origin_narration = json.load(open(origin_path))['videos']\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "    \n",
    "    def get_origin_narration(self):\n",
    "        return self.origin_narration\n",
    "\n",
    "class BetaAlphaCalculator:\n",
    "    def __init__(self, data, alpha=4.9):\n",
    "        self.data = data\n",
    "        self.beta_map = {}\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def compute_beta(self):\n",
    "        for video_uid, annotation_uid_narrations in self.data.items():\n",
    "            for annotation_uid, narrations in annotation_uid_narrations.items():\n",
    "                if len(narrations) == 0:\n",
    "                    continue\n",
    "                total_time = 0\n",
    "                for i in range(len(narrations) - 1):\n",
    "                    total_time += narrations[i+1]['time'] - narrations[i]['time']\n",
    "                self.beta_map[annotation_uid] = total_time / len(narrations)\n",
    "    \n",
    "    def get_beta_map(self):\n",
    "        return self.beta_map\n",
    "    \n",
    "    def get_alpha(self):\n",
    "        return self.alpha\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, data, origin_narration, beta_map, alpha, video_root, frame_fps=2):\n",
    "        self.data = data\n",
    "        self.origin_narration = origin_narration\n",
    "        self.beta_map = beta_map\n",
    "        self.alpha = alpha\n",
    "        self.video_root = video_root\n",
    "        self.frame_fps = frame_fps\n",
    "        \n",
    "        from siglip import visionTextAligner\n",
    "        self.aliger = visionTextAligner()\n",
    "    \n",
    "    \n",
    "    def load_scene_clipv2(self, path, clip_idx, max_frame=32,):\n",
    "\n",
    "        annotation_uids = list(self.data[path].keys())\n",
    "        clip_id = annotation_uids[clip_idx]\n",
    "        \n",
    "        # load clip\n",
    "        summs = self.origin_narration[path]['summaries']\n",
    "        for summ in summs:\n",
    "            if summ['_annotation_uid'] == clip_id:\n",
    "                break\n",
    "            \n",
    "        start_time, end_time = summ['start_time'], summ['end_time']\n",
    "        vr = VideoReader(uri=os.path.join(self.video_root, path) + '.mp4')\n",
    "        start_frame = int(ceil_time_by_fps(start_time, self.frame_fps, 0, vr._num_frame / self.frame_fps) * self.frame_fps)\n",
    "        end_frame = int(ceil_time_by_fps(end_time, self.frame_fps, 0, vr._num_frame / self.frame_fps)* self.frame_fps) + 1\n",
    "        load_range = range(start_frame, end_frame)\n",
    "        frames = vr.get_batch(load_range)\n",
    "        \n",
    "        # vision simi\n",
    "        simi = self.aliger.vision_simi(frames)\n",
    "        frames = vr.get_batch(load_range)\n",
    "        frames = [Image.fromarray(v.astype('uint8')) for v in frames.numpy()]\n",
    "        if simi > 0.8:\n",
    "            if len(frames) > max_frame:\n",
    "                # uniformly sample frames\n",
    "                step = math.ceil(len(frames) / max_frame)\n",
    "                frames = frames[::step]\n",
    "                # save frame info\n",
    "                \n",
    "                frames = VideoProcessor.add_frame_info(frames, start_frame, self.frame_fps / step)\n",
    "                \n",
    "                load_range = range(0,len(frames))\n",
    "                yield frames, start_frame, end_frame, load_range, (self.frame_fps / step), simi\n",
    "        elif simi < 0.6:\n",
    "            for i in range(0, len(frames), max_frame):\n",
    "                r_f = frames[i:i+max_frame]\n",
    "                \n",
    "                r_f = VideoProcessor.add_frame_info(r_f, start_time + i / self.frame_fps, self.frame_fps)\n",
    "                yield r_f, start_frame + i, start_frame + (i + len(r_f)), range(0,len(r_f)), self.frame_fps, simi\n",
    "        else:\n",
    "            step = 1\n",
    "            if len(frames) > max_frame:\n",
    "                # uniformly sample frames\n",
    "                frames = frames[::self.frame_fps*2]\n",
    "                step = (self.frame_fps*2)\n",
    "            for i in range(0, len(frames), max_frame):\n",
    "                r_f = frames[i:i+max_frame]\n",
    "                \n",
    "                r_f = VideoProcessor.add_frame_info(r_f, start_time + i / self.frame_fps, self.frame_fps / step)\n",
    "                yield r_f, start_frame + i, start_frame + (i + len(r_f)) / self.frame_fps, range(0,len(r_f)), (self.frame_fps / step), simi\n",
    "\n",
    "    @staticmethod\n",
    "    def add_frame_info(frames, start_frame, frame_fps):\n",
    "        \"\"\"\n",
    "        在每一帧左上角添加时间或帧编号信息。\n",
    "        \n",
    "        参数:\n",
    "            frames (list): 视频帧列表，每一帧是一个 PIL Image 对象。\n",
    "            start_frame (int): 起始帧编号。\n",
    "            frame_fps (int): 视频的帧率。\n",
    "        \n",
    "        返回:\n",
    "            list: 添加了信息的帧列表。\n",
    "        \"\"\"\n",
    "        font = ImageFont.truetype(\"/root/videollm-online/data/preprocess/font/ARIAL.TTF\", size=100) \n",
    "        annotated_frames = []\n",
    "        for idx, frame in enumerate(frames):\n",
    "            current_frame = start_frame + idx\n",
    "            time_seconds = current_frame / frame_fps\n",
    "            time_text = f\"{time_seconds:.2f}s\"  # 格式化为秒的小数形式\n",
    "            frame_text = f\"Frame {current_frame}\"  # 显示帧编号\n",
    "            \n",
    "\n",
    "            draw = ImageDraw.Draw(frame)\n",
    "            draw.text((10, 10), time_text, fill=\"red\", font=font)\n",
    "            draw.text((10, 200), frame_text, fill=\"red\", font=font)\n",
    "            \n",
    "            annotated_frames.append(frame)\n",
    "        \n",
    "        return annotated_frames\n",
    "            \n",
    "\n",
    "class CaptionGenerator:\n",
    "    def __init__(self, model_name, tokenizer_name, device='cuda:4', dtype=torch.bfloat16):\n",
    "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                               attn_implementation='sdpa', torch_dtype=dtype)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "            \n",
    "    def get_caption(self, frames, question):\n",
    "        msgs = [\n",
    "            {'role': 'user', 'content': frames + [question]}, \n",
    "        ]\n",
    "        \n",
    "        # Set decode params for video\n",
    "        params={}\n",
    "        params[\"use_image_id\"] = False\n",
    "        params[\"max_slice_nums\"] = 2 # use 1 if cuda OOM and video resolution >  448*448\n",
    "\n",
    "        answer = self.model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=self.tokenizer,\n",
    "            **params\n",
    "        )\n",
    "        return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization time: 188.80s, start captioning...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "    \n",
    "time1 = time.time()\n",
    "\n",
    "train_path = '/root/videollm-online/datasets/ego4d/v2/annotations/refined_narration_stream_train.json'\n",
    "val_path = '/root/videollm-online/datasets/ego4d/v2/annotations/refined_narration_stream_val.json'\n",
    "origin_path = '/root/videollm-online/datasets/ego4d/v2/annotations/all_narrations_redacted.json'\n",
    "video_root = '/root/videollm-online/datasets/ego4d/v2/full_scale_2fps'\n",
    "video2scene = json.load(open('/root/videollm-online/data/preprocess/metafile/video2scene.json'))\n",
    "video_uid_list = open('/root/videollm-online/data/preprocess/metafile/major2scene_case.txt').read().split('\\n')\n",
    "alpha = 4.9\n",
    "device = 'cuda:5'\n",
    "\n",
    "# 初始化各个模块\n",
    "annotation_loader = AnnotationLoader(train_path, val_path, origin_path)\n",
    "data = annotation_loader.get_data()\n",
    "origin_narration = annotation_loader.get_origin_narration()\n",
    "\n",
    "beta_alpha_calculator = BetaAlphaCalculator(data, alpha)\n",
    "beta_alpha_calculator.compute_beta()\n",
    "beta_map = beta_alpha_calculator.get_beta_map()\n",
    "alpha = beta_alpha_calculator.get_alpha()\n",
    "\n",
    "video_processor = VideoProcessor(data, origin_narration, beta_map, alpha, video_root)\n",
    "caption_generator = CaptionGenerator('openbmb/MiniCPM-V-2_6', 'openbmb/MiniCPM-V-2_6', device=device)\n",
    "\n",
    "print(f'Initialization time: {time.time() - time1:.2f}s, start captioning...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptGenerator:\n",
    "    def __init__(self, prompt_file):\n",
    "        self.prompt = open(prompt_file).read()\n",
    "    \n",
    "    \n",
    "class PromptGeneratorQA(PromptGenerator):\n",
    "    def __init__(self, prompt_file):\n",
    "        super().__init__(prompt_file)\n",
    "    \n",
    "    def get_prompt(self, question):\n",
    "        return self.prompt.format(question)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = 'qa_output'\n",
    "path = '000cd456-ff8d-499b-b0c1-4acead128a8b'\n",
    "prompt_file = '/root/videollm-online/data/preprocess/prompt/QA_time.txt'\n",
    "clip_idx = 0\n",
    "annotation_uid_narrations = data[path]\n",
    "annotation_uid = list(annotation_uid_narrations.keys())[clip_idx]\n",
    "\n",
    "subject = ' / '.join(video2scene[path])\n",
    "clip_gen = video_processor.load_scene_clipv2(path, clip_idx)\n",
    "\n",
    "origin_question = \"What is the distinctive feature of the white cat ?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will receive a video input where each frame displays a timestamp and the frame number in the top-left corner. I will ask you questions about the video, and you need to answer them. When relevant, identify the first frame number and timestamp where the answer is visually supported in the video.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Question: What colour is the coat of the man on my right?\n",
      "Answer: The color of the coat is blue. The answer appears for the first time in frame 10. \n",
      "\n",
      "Question: What is the primary object the person is using to interact with the cats in the video? \n",
      "Answer: It is a colorful feathered toy attached to a black stick. The answer appears for the first time in frame 50.\n",
      "\n",
      "Your Turn:\n",
      "\n",
      "Question: Can you identify the time segment in the video where I started to interact with the cats using colorful feathered toy attached to a black stick?\n",
      "Answer: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/videollm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "0it [05:46, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the video shows that you started interacting with the cats using a colorful feathered toy attached to a black stick around 3 seconds into the footage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "origin_question = \"Can you identify the time segment in the video where I started to interact with the cats using colorful feathered toy attached to a black stick? (You can refer to the timestamp and frame number in the upper-left corner)\"\n",
    "prompt_genrator = PromptGeneratorQA(prompt_file)\n",
    "\n",
    "for action_idx, (frames, start_frame, end_frame, load_range, fps, simi) in tqdm(enumerate(clip_gen)):\n",
    "    # os.makedirs(f'{output_dir}/{path}/{annotation_uid}', exist_ok=True)\n",
    "    # show_image(load_range, frames, f'{output_dir}/{path}/{annotation_uid}/{action_idx}.png')\n",
    "\n",
    "    question = prompt_genrator.get_prompt(origin_question)\n",
    "    print(question)\n",
    "    question, answer = caption_generator.get_caption(frames, origin_question)\n",
    "\n",
    "    print(answer)\n",
    "    # with open(f'{output_dir}/{path}/{annotation_uid}/{action_idx}.txt', 'w') as f:\n",
    "    #     f.write(question + '\\n')\n",
    "    #     f.write(answer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will receive a video input where each frame displays a timestamp and the frame number in the top-left corner. I will ask you questions about the video, and you need to answer them. When relevant, identify the first frame number and timestamp where the answer is visually supported in the video.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Question: What colour is the coat of the man on my right?\n",
      "Answer: The color of the coat is blue. The answer appears for the first time in frame 10. \n",
      "\n",
      "Question: What is the primary object the person is using to interact with the cats in the video? \n",
      "Answer: It is a colorful feathered toy attached to a black stick. The answer appears for the first time in frame 50.\n",
      "\n",
      "Your Turn:\n",
      "\n",
      "Question: Can you identify the time segment in the video where I started to interact with the cats using colorful feathered toy attached to a black stick? (You can refer to the timestamp and frame number in the upper-left corner)\n",
      "Answer: \n",
      "\n",
      "The first frame where the person starts interacting with the cats using a colorful feathered toy attached to a black stick is in frame 50 at approximately 62.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "origin_question = \"Can you identify the time segment in the video where I started to interact with the cats using colorful feathered toy attached to a black stick? (You can refer to the timestamp and frame number in the upper-left corner)\"\n",
    "question = prompt_genrator.get_prompt(origin_question)\n",
    "print(question)\n",
    "question, answer = caption_generator.get_caption(frames, question)\n",
    "\n",
    "print(answer)\n",
    "# with open(f'{output_dir}/{path}/{annotation_uid}/{action_idx}.txt', 'w') as f:\n",
    "#     f.write(question + '\\n')\n",
    "#     f.write(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
