{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EGO_ROOT = '/home/zhangyl/videollm-online/datasets/ego4d/'\n",
    "CAPTION_ROOT = '/home/zhangyl/videollm-online/datasets/ego4d_action_caption'\n",
    "MOVE_CAPTION_ROOT = '/home/zhangyl/videollm-online/datasets/ego4d_move_action_caption'\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from video_caption_action_scene import AnnotationLoader, BetaAlphaCalculator\n",
    "\n",
    "EGO_VERSION_ROOT = os.path.join(EGO_ROOT, 'v2')\n",
    "json_path = os.path.join(EGO_ROOT, 'ego4d.json')\n",
    "train_path = f'{EGO_VERSION_ROOT}/annotations/refined_narration_stream_train.json'\n",
    "val_path = f'{EGO_VERSION_ROOT}/annotations/refined_narration_stream_val.json'\n",
    "origin_path = f'{EGO_VERSION_ROOT}/annotations/all_narrations_redacted.json'\n",
    "video_root = f'{EGO_VERSION_ROOT}/full_scale_2fps'\n",
    "alpha = 4.9\n",
    "device = 'cuda:3'\n",
    "caption_dir = '/root/videollm-online/tmp5'\n",
    "\n",
    "annotation_loader = AnnotationLoader(train_path, val_path, origin_path, json_path)\n",
    "data = annotation_loader.get_data()\n",
    "origin_narration = annotation_loader.get_origin_narration()\n",
    "\n",
    "beta_alpha_calculator = BetaAlphaCalculator(data, alpha)\n",
    "beta_alpha_calculator.compute_beta()\n",
    "beta_map = beta_alpha_calculator.get_beta_map()\n",
    "alpha = beta_alpha_calculator.get_alpha()\n",
    "\n",
    "train_caption = json.load(open(f'{CAPTION_ROOT}/action_caption_train.json'))\n",
    "val_caption = json.load(open(f'{CAPTION_ROOT}/action_caption_val.json'))\n",
    "all_caption = {**train_caption, **val_caption}\n",
    "\n",
    "move_train_caption = json.load(open(f'{MOVE_CAPTION_ROOT}/action_caption_train.json'))\n",
    "move_val_caption = json.load(open(f'{MOVE_CAPTION_ROOT}/action_caption_val.json'))\n",
    "move_all_caption = {**move_train_caption, **move_val_caption}\n",
    "video2scene = json.load(open('/home/zhangyl/videollm-online/data/estp/ego4d/metafile/video2scene.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# object QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## move action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_caption_wo_action(captions, video_uid, clip_uid, video2scene, origin_narration, w_pre = False):\n",
    "    caption = captions[video_uid][clip_uid]\n",
    "    caption_texts = {}\n",
    "    for action_idx, cap in enumerate(caption):\n",
    "        \n",
    "        caption_texts[action_idx] = {\n",
    "            'caption': cap['caption'],\n",
    "        }\n",
    "        \n",
    "    return caption_texts, caption\n",
    "\n",
    "def print_json(caption_texts):\n",
    "    print(json.dumps(caption_texts, indent=4))\n",
    "    \n",
    "def caption_merger(captions, video2scene, origin_narration, w_pre = False):\n",
    "    for video_uid in captions.keys():\n",
    "        for clip_uid in captions[video_uid].keys():\n",
    "            caption_texts, caption = merge_caption_wo_action(captions, video_uid, clip_uid, video2scene, origin_narration, w_pre = False)\n",
    "            yield caption_texts, caption, video_uid, clip_uid\n",
    "\n",
    "caption_merger1 = caption_merger(move_all_caption, video2scene, origin_narration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"caption\": \"The video showcases a first-person perspective of an individual engaged in a mechanical or automotive task within a workshop environment. The primary focus is on the person's hands, which are clad in black gloves with red accents and occasionally hold various tools and parts.\\n\\nInitially, the individual is seen holding a large cylindrical object, likely a part of a vehicle such as a headlight or similar component, against a backdrop of a cluttered workbench covered with a white cloth stained with grease or paint. Tools and materials are scattered around, indicating an active workspace.\\n\\nAs the video progresses, the individual manipulates the cylindrical object, cleaning it with a yellow cloth. The movement is methodical, suggesting careful handling to ensure thorough cleaning without damaging the surface. The camera remains steady throughout these actions, providing a consistent view of the process.\\n\\nSubsequently, the scene shifts slightly to show the person interacting with different objects on the bench, including what appears to be a metal part with circular holes, possibly another vehicle component like a housing for lights or sensors. The positioning of the objects relative to the observer changes as they are moved closer for inspection or cleaned.\\n\\nThroughout the video, there is a notable interaction between the person and the items, involving both manual manipulation and the use of tools. For instance, the application of a spray can suggests the use of lubricants or cleaners, adding another layer of maintenance activity to the scene.\\n\\nThe camera captures these interactions from a fixed point of view, ensuring that the viewer can observe the detailed steps taken by the individual. Despite minor adjustments in framing due to the movements of the person\\u2019s arms, the overall setting remains constant\\u2014a busy workshop filled with tools and parts necessary for mechanical work.\\n\\nIn summary, the video depicts a series of meticulous tasks performed by an individual in a workshop environment, focusing on cleaning and maintaining automotive components. The positioning of objects, their relative spatial relationships, and the interactions captured through the camera's perspective all contribute to portraying a comprehensive view of the mechanical work being undertaken.\",\n",
      "        \"start_time\": 0.0210286,\n",
      "        \"end_time\": 274.48828273153157,\n",
      "        \"pre_scene\": true,\n",
      "        \"text\": \"The video showcases a first-person perspective of an individual engaged in a mechanical or automotive task within a workshop environment. The primary focus is on the person's hands, which are clad in black gloves with red accents and occasionally hold various tools and parts.\\n\\nInitially, the individual is seen holding a large cylindrical object, likely a part of a vehicle such as a headlight or similar component, against a backdrop of a cluttered workbench covered with a white cloth stained with grease or paint. Tools and materials are scattered around, indicating an active workspace.\\n\\nAs the video progresses, the individual manipulates the cylindrical object, cleaning it with a yellow cloth. The movement is methodical, suggesting careful handling to ensure thorough cleaning without damaging the surface. The camera remains steady throughout these actions, providing a consistent view of the process.\\n\\nSubsequently, the scene shifts slightly to show the person interacting with different objects on the bench, including what appears to be a metal part with circular holes, possibly another vehicle component like a housing for lights or sensors. The positioning of the objects relative to the observer changes as they are moved closer for inspection or cleaned.\\n\\nThroughout the video, there is a notable interaction between the person and the items, involving both manual manipulation and the use of tools. For instance, the application of a spray can suggests the use of lubricants or cleaners, adding another layer of maintenance activity to the scene.\\n\\nThe camera captures these interactions from a fixed point of view, ensuring that the viewer can observe the detailed steps taken by the individual. Despite minor adjustments in framing due to the movements of the person\\u2019s arms, the overall setting remains constant\\u2014a busy workshop filled with tools and parts necessary for mechanical work.\\n\\nIn summary, the video depicts a series of meticulous tasks performed by an individual in a workshop environment, focusing on cleaning and maintaining automotive components. The positioning of objects, their relative spatial relationships, and the interactions captured through the camera's perspective all contribute to portraying a comprehensive view of the mechanical work being undertaken.\",\n",
      "        \"time\": 274.48828273153157,\n",
      "        \"obj_list\": [\n",
      "            \"black gloves with red accents\",\n",
      "            \"large cylindrical object (vehicle part)\",\n",
      "            \"white cloth-covered workbench\",\n",
      "            \"yellow cloth\",\n",
      "            \"metal part with circular holes (vehicle component)\",\n",
      "            \"spray can (lubricant or cleaner)\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"caption\": \"The video begins with a first-person perspective in what appears to be a cluttered workshop or garage. The camera is held by someone wearing black gloves, and the initial focus is on their hands as they manipulate a metallic object, possibly a part of machinery or a tool, which they hold above a white cloth-covered surface.\\n\\nAs the video progresses, the person moves away from this immediate task, resulting in a blurred view that indicates rapid movement. This motion suggests that the individual is walking through the space, causing the objects around them to shift relative to the camera's position. \\n\\nThe environment reveals various tools and equipment scattered throughout the room. To the left, there are shelves holding items like drills, pliers, and other hand tools, along with some containers and boxes. On the right side, more tools and materials can be seen, including what looks like a red toolbox and additional storage units. A car is parked towards the back, partially visible through an open door leading outside, suggesting the setting could be a home garage.\\n\\nThroughout the clip, the camera continues to move, capturing different angles and perspectives within the workspace. At one point, it seems to focus on the floor, showing debris and a cardboard box, indicating ongoing work or recent activity in the area. The presence of chairs and additional tables further emphasizes the multifunctional nature of the space, used for both seating and work-related tasks.\\n\\nIn summary, the scene captures a dynamic and active workspace where objects such as tools, machinery parts, and vehicles are present. The movements of the observer create a sense of exploration and interaction within this environment, highlighting the spatial relationships between various elements and the general state of disarray typical of a busy workshop or garage.\",\n",
      "        \"stamp_time\": 276.0983038666667,\n",
      "        \"start_time\": 274.48828273153157,\n",
      "        \"end_time\": 277.7083250018018,\n",
      "        \"simi\": 0.7199358344078064,\n",
      "        \"action_narration\": \"You walk on the house.\",\n",
      "        \"action_idx\": 28,\n",
      "        \"text\": \"The video begins with a first-person perspective in what appears to be a cluttered workshop or garage. The camera is held by someone wearing black gloves, and the initial focus is on their hands as they manipulate a metallic object, possibly a part of machinery or a tool, which they hold above a white cloth-covered surface.\\n\\nAs the video progresses, the person moves away from this immediate task, resulting in a blurred view that indicates rapid movement. This motion suggests that the individual is walking through the space, causing the objects around them to shift relative to the camera's position. \\n\\nThe environment reveals various tools and equipment scattered throughout the room. To the left, there are shelves holding items like drills, pliers, and other hand tools, along with some containers and boxes. On the right side, more tools and materials can be seen, including what looks like a red toolbox and additional storage units. A car is parked towards the back, partially visible through an open door leading outside, suggesting the setting could be a home garage.\\n\\nThroughout the clip, the camera continues to move, capturing different angles and perspectives within the workspace. At one point, it seems to focus on the floor, showing debris and a cardboard box, indicating ongoing work or recent activity in the area. The presence of chairs and additional tables further emphasizes the multifunctional nature of the space, used for both seating and work-related tasks.\\n\\nIn summary, the scene captures a dynamic and active workspace where objects such as tools, machinery parts, and vehicles are present. The movements of the observer create a sense of exploration and interaction within this environment, highlighting the spatial relationships between various elements and the general state of disarray typical of a busy workshop or garage.\",\n",
      "        \"time\": 277.7083250018018,\n",
      "        \"obj_list\": [\n",
      "            \"black gloves\",\n",
      "            \"metallic object (machinery part or tool)\",\n",
      "            \"white cloth-covered surface\",\n",
      "            \"shelves with drills, pliers, hand tools\",\n",
      "            \"containers and boxes\",\n",
      "            \"red toolbox\",\n",
      "            \"car\",\n",
      "            \"cardboard box\",\n",
      "            \"chairs\",\n",
      "            \"tables\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"caption\": \"The video depicts a first-person perspective of an individual navigating through a cluttered workshop or garage space. Initially, the camera captures a wide view showing various objects and tools scattered around the room, including a workbench with electronic equipment, a computer monitor, and other miscellaneous items on shelves and the floor.\\n\\nAs the video progresses, the person moves closer to the workbench, focusing on different parts of it. The bench is equipped with various tools and components, suggesting it's used for mechanical or technical tasks. A green folding chair is visible to the left side of the frame, and a red toolbox can be seen in the background on the right.\\n\\nThe individual's hands come into view, wearing black gloves with red accents, indicating they are handling something. They pick up a metallic object that appears to be a part of machinery or equipment, possibly related to automotive or industrial use. This object is held up close to the camera, showcasing its features such as cylindrical shapes and metal construction.\\n\\nThroughout the video, the camera follows the movements closely, panning and tilting to provide clear views of the object being examined. The environment remains consistent with the same lighting and spatial arrangement of objects, but the focus shifts slightly as the person manipulates the item, rotating it to show different angles.\\n\\nThe interaction involves careful examination and potential assembly or disassembly of the mechanical component, emphasizing the hands-on nature of the task within the workspace. The camera movement is deliberate, ensuring each aspect of the object is visible and highlighting its details against the backdrop of the busy workshop setting.\",\n",
      "        \"start_time\": 277.7083250018018,\n",
      "        \"end_time\": 283.49903273153154,\n",
      "        \"pre_scene\": true,\n",
      "        \"text\": \"The video depicts a first-person perspective of an individual navigating through a cluttered workshop or garage space. Initially, the camera captures a wide view showing various objects and tools scattered around the room, including a workbench with electronic equipment, a computer monitor, and other miscellaneous items on shelves and the floor.\\n\\nAs the video progresses, the person moves closer to the workbench, focusing on different parts of it. The bench is equipped with various tools and components, suggesting it's used for mechanical or technical tasks. A green folding chair is visible to the left side of the frame, and a red toolbox can be seen in the background on the right.\\n\\nThe individual's hands come into view, wearing black gloves with red accents, indicating they are handling something. They pick up a metallic object that appears to be a part of machinery or equipment, possibly related to automotive or industrial use. This object is held up close to the camera, showcasing its features such as cylindrical shapes and metal construction.\\n\\nThroughout the video, the camera follows the movements closely, panning and tilting to provide clear views of the object being examined. The environment remains consistent with the same lighting and spatial arrangement of objects, but the focus shifts slightly as the person manipulates the item, rotating it to show different angles.\\n\\nThe interaction involves careful examination and potential assembly or disassembly of the mechanical component, emphasizing the hands-on nature of the task within the workspace. The camera movement is deliberate, ensuring each aspect of the object is visible and highlighting its details against the backdrop of the busy workshop setting.\",\n",
      "        \"time\": 283.49903273153154,\n",
      "        \"obj_list\": [\n",
      "            \"workbench with electronic equipment\",\n",
      "            \"computer monitor\",\n",
      "            \"green folding chair\",\n",
      "            \"red toolbox\",\n",
      "            \"black gloves with red accents\",\n",
      "            \"metallic object (machinery or equipment part)\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"caption\": \"The video appears to be shot in a cluttered workshop or garage environment. The camera provides an ego-perspective, suggesting it is mounted on the person's head or chest.\\n\\n1. **Object Positioning**:\\n   - A metal object with multiple cylindrical components is held by the observer, positioned centrally and slightly towards the bottom of the frame.\\n   - To the left, there are various items such as tools, cables, and what looks like a white cloth or towel draped over a workbench or counter.\\n   - In the background, more tools and equipment are scattered around, indicating an active workspace.\\n\\n2. **Relative Positioning of Objects**:\\n   - The metal object is being moved across the scene, likely from one area to another within the workspace.\\n   - The white cloth is on the table to the left, and other objects are placed around it, including what seems to be a cardboard box near the center-left of the frame.\\n   - Cables and wires run along the floor, leading to different parts of the room, indicating connectivity between various devices or tools.\\n\\n3. **Camera Movement and Object Position Change**:\\n   - As the video progresses, the camera moves forward, giving a clearer view of the surroundings and the metal object being handled.\\n   - The positioning of the metal object changes relative to the camera; at times, it is closer to the viewer, making its details more visible.\\n   - The movement suggests that the observer is walking while handling the object, possibly moving towards a specific location within the workspace for further inspection or assembly.\\n\\n4. **Interactions and Movements**:\\n   - There is no direct interaction with other objects beyond the manipulation of the metal object.\\n   - The presence of the gloves indicates careful handling, likely due to the object's delicate nature or the need for protection.\\n   - The environment remains static except for the movement of the camera and the object, which implies focused activity centered around this task.\\n\\nOverall, the video captures a moment in a busy workspace where the primary action involves manipulating a metal component, with the surrounding environment providing context to the setting.\",\n",
      "        \"stamp_time\": 285.10905386666667,\n",
      "        \"start_time\": 283.49903273153154,\n",
      "        \"end_time\": 286.7190750018018,\n",
      "        \"simi\": 0.6948096752166748,\n",
      "        \"action_narration\": \"You put the manifolds on the table.\",\n",
      "        \"action_idx\": 30,\n",
      "        \"text\": \"The video appears to be shot in a cluttered workshop or garage environment. The camera provides an ego-perspective, suggesting it is mounted on the person's head or chest.\\n\\n1. **Object Positioning**:\\n   - A metal object with multiple cylindrical components is held by the observer, positioned centrally and slightly towards the bottom of the frame.\\n   - To the left, there are various items such as tools, cables, and what looks like a white cloth or towel draped over a workbench or counter.\\n   - In the background, more tools and equipment are scattered around, indicating an active workspace.\\n\\n2. **Relative Positioning of Objects**:\\n   - The metal object is being moved across the scene, likely from one area to another within the workspace.\\n   - The white cloth is on the table to the left, and other objects are placed around it, including what seems to be a cardboard box near the center-left of the frame.\\n   - Cables and wires run along the floor, leading to different parts of the room, indicating connectivity between various devices or tools.\\n\\n3. **Camera Movement and Object Position Change**:\\n   - As the video progresses, the camera moves forward, giving a clearer view of the surroundings and the metal object being handled.\\n   - The positioning of the metal object changes relative to the camera; at times, it is closer to the viewer, making its details more visible.\\n   - The movement suggests that the observer is walking while handling the object, possibly moving towards a specific location within the workspace for further inspection or assembly.\\n\\n4. **Interactions and Movements**:\\n   - There is no direct interaction with other objects beyond the manipulation of the metal object.\\n   - The presence of the gloves indicates careful handling, likely due to the object's delicate nature or the need for protection.\\n   - The environment remains static except for the movement of the camera and the object, which implies focused activity centered around this task.\\n\\nOverall, the video captures a moment in a busy workspace where the primary action involves manipulating a metal component, with the surrounding environment providing context to the setting.\",\n",
      "        \"time\": 286.7190750018018,\n",
      "        \"obj_list\": [\n",
      "            \"metal object with multiple cylindrical components\",\n",
      "            \"tools\",\n",
      "            \"cables\",\n",
      "            \"white cloth or towel\",\n",
      "            \"cardboard box\",\n",
      "            \"cables and wires\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"caption\": \"The video begins with a first-person perspective, likely from a camera mounted on the person's head or chest, capturing their feet as they walk across an industrial or workshop environment. The floor is concrete, and various items are scattered around, including tools, boxes, and equipment.\\n\\nAs the individual moves forward, they pass by a cluttered area with a red toolbox filled with assorted tools, a cardboard box, and some loose wires. The space appears to be a garage or workshop, characterized by its utilitarian setup and lack of organization.\\n\\nThe camera then shifts slightly, revealing more of the room, including a workbench covered with various items such as automotive parts, containers, and a roll of tape. A pegboard with hanging tools is visible in the background, indicating that this area is used for mechanical or repair tasks.\\n\\nContinuing along the path, the person approaches the workbench more closely, focusing on different objects placed on it. Items like metal components, a spray can, and gloves become clearer as the camera zooms in. The movement suggests the person is preparing to engage in some form of assembly or repair work.\\n\\nThroughout the video, the camera remains steady, offering a consistent viewpoint of the workspace. Objects like a power drill, a piece of machinery, and additional automotive parts come into focus as the individual interacts with them, possibly assembling or disassembling something.\\n\\nThe person's hands are frequently seen manipulating these objects, suggesting active engagement in a task. The positioning of the hands relative to the camera indicates close-up interactions with the items on the bench.\\n\\nIn summary, the video documents a sequence where the observer navigates through a cluttered workshop, focusing on detailed interactions with tools and components on a workbench. The environment is characterized by a mix of automotive and general repair tools, and the camera provides a continuous, intimate view of the activities taking place within this setting.\",\n",
      "        \"start_time\": 286.7190750018018,\n",
      "        \"end_time\": 296.25261273153154,\n",
      "        \"pre_scene\": true,\n",
      "        \"text\": \"The video begins with a first-person perspective, likely from a camera mounted on the person's head or chest, capturing their feet as they walk across an industrial or workshop environment. The floor is concrete, and various items are scattered around, including tools, boxes, and equipment.\\n\\nAs the individual moves forward, they pass by a cluttered area with a red toolbox filled with assorted tools, a cardboard box, and some loose wires. The space appears to be a garage or workshop, characterized by its utilitarian setup and lack of organization.\\n\\nThe camera then shifts slightly, revealing more of the room, including a workbench covered with various items such as automotive parts, containers, and a roll of tape. A pegboard with hanging tools is visible in the background, indicating that this area is used for mechanical or repair tasks.\\n\\nContinuing along the path, the person approaches the workbench more closely, focusing on different objects placed on it. Items like metal components, a spray can, and gloves become clearer as the camera zooms in. The movement suggests the person is preparing to engage in some form of assembly or repair work.\\n\\nThroughout the video, the camera remains steady, offering a consistent viewpoint of the workspace. Objects like a power drill, a piece of machinery, and additional automotive parts come into focus as the individual interacts with them, possibly assembling or disassembling something.\\n\\nThe person's hands are frequently seen manipulating these objects, suggesting active engagement in a task. The positioning of the hands relative to the camera indicates close-up interactions with the items on the bench.\\n\\nIn summary, the video documents a sequence where the observer navigates through a cluttered workshop, focusing on detailed interactions with tools and components on a workbench. The environment is characterized by a mix of automotive and general repair tools, and the camera provides a continuous, intimate view of the activities taking place within this setting.\",\n",
      "        \"time\": 296.25261273153154,\n",
      "        \"obj_list\": [\n",
      "            \"concrete floor\",\n",
      "            \"tools\",\n",
      "            \"boxes\",\n",
      "            \"equipment\",\n",
      "            \"red toolbox\",\n",
      "            \"cardboard box\",\n",
      "            \"loose wires\",\n",
      "            \"workbench with automotive parts, containers, roll of tape\",\n",
      "            \"pegboard with hanging tools\",\n",
      "            \"metal components\",\n",
      "            \"spray can\",\n",
      "            \"gloves\",\n",
      "            \"power drill\",\n",
      "            \"piece of machinery\",\n",
      "            \"additional automotive parts\"\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def get_llm_response_json(system_prompt, user_prompt):\n",
    "    client = OpenAI(\n",
    "            api_key=\"\",\n",
    "            base_url=\"https://api.deepseek.com\",\n",
    "        )\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=messages,\n",
    "        response_format={\n",
    "            'type': 'json_object'\n",
    "        }\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "system_prompt = open('/home/zhangyl/videollm-online/data/estp/ego4d/prompt/caption2object_system_prompt.txt').read().format(NUMBER=2)\n",
    "user_prompt_templete = open('/home/zhangyl/videollm-online/data/estp/ego4d/prompt/caption2qa_prompt.txt').read()\n",
    "output_dir = '/home/zhangyl/videollm-online/dataset/move_caption2object_v1/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for i in range(0,1):\n",
    "\n",
    "    caption_texts, caption, video_uid, clip_uid = next(caption_merger1)\n",
    "\n",
    "    user_prompt = user_prompt_templete.format(json.dumps(caption_texts,indent=4))\n",
    "    # answer = (get_llm_response_json(system_prompt, user_prompt))\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'{video_uid}_{clip_uid}_caption.txt'), 'w') as f:\n",
    "        f.write(user_prompt)\n",
    "    # with open(os.path.join(output_dir, f'{video_uid}_{clip_uid}_object.txt'), 'w') as f:\n",
    "    #     f.write(answer)\n",
    "    \n",
    "    # answer_json = json.loads(answer)\n",
    "    # for cap, obj in zip(caption, answer_json.values()):\n",
    "    #     cap['obj_list'] = obj['object_list']\n",
    "    \n",
    "    \n",
    "    # user_prompt = user_prompt_templete_a.format(caption_texts, answer)\n",
    "    # answer = get_llm_response_json(system_prompt_a, user_prompt)\n",
    "    \n",
    "    # with open(os.path.join(output_dir, f'{video_uid}_{clip_uid}_caption2.txt'), 'w') as f:\n",
    "    #     f.write(user_prompt)\n",
    "    # with open(os.path.join(output_dir, f'{video_uid}_{clip_uid}_a.txt'), 'w') as f:\n",
    "    #     f.write(answer)\n",
    "    \n",
    "print_json(caption)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cc9a153-cec3-4ec5-8e85-78c775653bd2 a003760a-fb37-4cbc-9ca8-6db79d3b0029\n"
     ]
    }
   ],
   "source": [
    "print(video_uid, clip_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## move action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_caption_wo_action(captions, video_uid, clip_uid, video2scene, origin_narration, w_pre = False):\n",
    "    caption = captions[video_uid][clip_uid]\n",
    "    \n",
    "    caption_texts = \"Video Subject : {}\\n\".format(' / '.join(video2scene[video_uid]))\n",
    "    for action_idx, cap in enumerate(caption):\n",
    "        \n",
    "        if 'stamp_time' not in cap and w_pre:\n",
    "            action_narration = 'Time is {} - {}.\\n'.format(cap['start_time'], cap['end_time'])\n",
    "        elif 'stamp_time' not in cap and not w_pre:\n",
    "            continue\n",
    "        else:\n",
    "            action_narration = 'Time is {}.\\n'.format(cap['stamp_time'])\n",
    "        caption_text = 'Detailed Description: \\\"' + cap['text'] + '\\\".\\n'\n",
    "        caption_text = action_narration + caption_text\n",
    "        caption_texts += caption_text + '\\n\\n'\n",
    "        \n",
    "    return caption_texts\n",
    "\n",
    "def caption_merger(captions, video2scene, origin_narration):\n",
    "    for video_uid in captions.keys():\n",
    "        for clip_uid in captions[video_uid].keys():\n",
    "            caption_texts = merge_caption_wo_action(captions, video_uid, clip_uid, video2scene, origin_narration)\n",
    "            yield caption_texts, video_uid, clip_uid\n",
    "\n",
    "caption_merger1 = caption_merger(move_all_caption, video2scene, origin_narration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def get_llm_response_json(system_prompt, user_prompt):\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=\"\",\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "system_prompt = open('/home/zhangyl/videollm-online/data/estp/ego4d/prompt/caption2qa_function_system.txt').read().format(NUMBER=2)\n",
    "user_prompt_templete = open('/home/zhangyl/videollm-online/data/estp/ego4d/prompt/caption2qa_function_prompt.txt').read()\n",
    "output_dir = '/home/zhangyl/videollm-online/dataset/move_action_function_v12/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for _ in range(0,10):\n",
    "    \n",
    "    caption_texts, video_uid, clip_uid = next(caption_merger1)\n",
    "    user_prompt = user_prompt_templete.format(caption_texts)\n",
    "    answer = get_llm_response_json(system_prompt, user_prompt)\n",
    "\n",
    "    with open(os.path.join(output_dir, f'{video_uid}_{clip_uid}_caption.txt'), 'w') as f:\n",
    "        f.write(user_prompt)\n",
    "    with open(os.path.join(output_dir, f'{video_uid}_{clip_uid}_qa.txt'), 'w') as f:\n",
    "        f.write(answer)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_caption_with_action(captions, narrations, video_uid, clip_uid, video2scene, origin_narration):\n",
    "    narration = narrations[video_uid][clip_uid]\n",
    "    caption = captions[video_uid][clip_uid]\n",
    "    \n",
    "    caption_texts = \"Video Subject : {}\\n\".format(' / '.join(video2scene[video_uid]))\n",
    "    for action_idx, (nar, cap) in enumerate(zip(narration, caption)):\n",
    "        action_narration = 'Time is {}. Action narration is \\\"'.format(nar['time']) + nar['text'] + '\\\".\\n'\n",
    "        caption_text = 'Detailed Description: \\\"' + cap['text'] + '\\\" \\n'\n",
    "        caption_text = action_narration + caption_text\n",
    "        caption_texts += caption_text + '\\n'\n",
    "        \n",
    "    return caption_texts\n",
    "\n",
    "def caption_merger_waction(captions, video2scene, origin_narration):\n",
    "    for video_uid in captions.keys():\n",
    "        for clip_uid in captions[video_uid].keys():\n",
    "            caption_texts = merge_caption_with_action(captions, data, video_uid, clip_uid, video2scene, origin_narration)\n",
    "            yield caption_texts, video_uid, clip_uid\n",
    "\n",
    "caption_merger2 = caption_merger_waction(all_caption, video2scene, origin_narration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Subject : BBQ'ing/picnics / Walking on street\n",
      "Time is 270.0. Action narration is \"You fold the luggage.\".\n",
      "Detailed Description: \"With a sense of purpose, you begin to fold the luggage. The beige fabric of the suitcase is well-worn but still sturdy as it sits on the floor in front of you. You methodically work through the steps: first, you gather the edges and bring them together, then you tuck in the straps, ensuring they are neatly tucked away. As you continue, the once sprawling mass of fabric gradually transforms into a compact, manageable size, ready for transport or storage.\" \n",
      "\n",
      "Time is 301.29231. Action narration is \"You look around.\".\n",
      "Detailed Description: \"As you look around, the room appears dimly lit with a warm, yellowish hue. The walls are painted in a light beige color, and there's a noticeable shadow cast by an unseen object or person. In front of you, on the floor, lies a large, brown blanket that seems to be spread out, possibly covering something underneath. To your right, part of a white furry pet is visible, its fur appearing soft and fluffy. On the far left side of the frame, a portion of a blue long-sleeved shirt can be seen worn by someone sitting down, their face obscured from view.\" \n",
      "\n",
      "Time is 365.99135. Action narration is \"She bags the bag.\".\n",
      "Detailed Description: \"As she bends down, her hand reaches into the white plastic bag. She meticulously sorts through its contents, picking out items and placing them neatly inside a brown paper bag. The room is dimly lit, casting soft shadows around her as she works diligently to organize the belongings in the bags.\" \n",
      "\n",
      "Time is 374.55773. Action narration is \"You lift the luggage and put it in the bag.\".\n",
      "Detailed Description: \"With a firm grip, you lift the large brown suitcase from the ground. You carefully position it on top of your beige shoulder bag, making sure that both items are securely placed and balanced for easy transport.\" \n",
      "\n",
      "Time is 412.69225. Action narration is \"She closes the bag zip.\".\n",
      "Detailed Description: \"With her right hand, she reaches for the beige bag that rests on her lap. She gently pulls at the zipper with a sense of purpose and care, ensuring it is securely fastened. The zipper glides smoothly over the fabric, and as she completes the action, she turns to face forward, her attention momentarily diverted from the task at hand.\" \n",
      "\n",
      "Time is 467.87971. Action narration is \"You lift the bag of luggage.\".\n",
      "Detailed Description: \"With a gentle yet firm grip, you lift the white bag of luggage from its resting place on the floor. The fabric of the bag appears slightly crinkled due to the weight it carries. As you hoist the bag, you notice that it is quite full, suggesting it contains various items for travel or storage.\" \n",
      "\n",
      "Time is 469.82164. Action narration is \"You give the bag of luggage to her.\".\n",
      "Detailed Description: \"With a gentle smile, you extend your hand towards the woman standing in front of you. The bag of luggage is securely held in your grasp. As you give it to her, she takes it with both hands and examines its contents before adjusting her grip. Her expression is one of gratitude and satisfaction as she prepares to transport the luggage elsewhere.\" \n",
      "\n",
      "Time is 470.75335. Action narration is \"She lifts the luggage.\".\n",
      "Detailed Description: \"With a determined expression, she approaches the large beige suitcase standing upright in the center of the room. Her hands grip the handle firmly and with practiced ease, she lifts it off the ground. The suitcase, sturdy and well-worn from frequent travels, responds to her strength as she maneuvers it across the tiled floor towards the open doorway leading outside.\" \n",
      "\n",
      "Time is 477.92945. Action narration is \"She gives back the luggage to you.\".\n",
      "Detailed Description: \"As you approach the woman, she hands over a large, light-colored luggage bag to you. The bag appears sturdy and spacious, suitable for travel. You take it from her with both hands, expressing gratitude as you hold it securely in front of you.\" \n",
      "\n",
      "Time is 479.61994000000004. Action narration is \"You carry the luggage.\".\n",
      "Detailed Description: \"As you carry the luggage, your left hand gently grasps its handle, supporting it as you walk. The suitcase is a medium-sized, dark-colored hard-shell with visible signs of wear and tear from frequent use. You move across the room, passing by various objects like a small black bag on the floor to your right and a white plastic chair in front of you. As you progress, the walls around you are adorned with framed pictures, adding a personal touch to the space. In the background, another person can be seen standing near an open doorway, observing your movements.\" \n",
      "\n",
      "Time is 481.93093999999996. Action narration is \"You move around.\".\n",
      "Detailed Description: \"As you move around, the room's layout becomes more apparent. The beige floor stretches out in front of you, leading to a set of black-framed glass doors that are slightly ajar. Through these doors, you can see another room with various objects scattered about, including what appears to be a white chair and some items on a table. To your right, there is a metal vent, suggesting an air conditioning or heating system. You notice a piece of paper lying on the floor near the door, and as you walk closer, you reach out and open the door fully.\" \n",
      "\n",
      "Time is 483.34427. Action narration is \"You close the door.\".\n",
      "Detailed Description: \"With a sense of purpose, you reach out and close the door gently. The metallic handle clicks into place as the door slides shut, sealing off the room from the outside world. As the door closes, the interior space becomes more intimate, with the sound of the door latch becoming muffled in the enclosed environment. You take a moment to assess your surroundings, taking in the details that make up this personal sanctuary.\" \n",
      "\n",
      "Time is 489.37571. Action narration is \"You get out of the house.\".\n",
      "Detailed Description: \"With a sense of urgency, you quickly move towards the front door, grabbing your keys from the side table as you go. You open the door and step outside, closing it behind you with a decisive click. The cool night air hits your face, and you take a deep breath before beginning to walk down the dimly lit street.\" \n",
      "\n",
      "Time is 496.77753. Action narration is \"You move along the street.\".\n",
      "Detailed Description: \"As you move along the street, the darkness of the night envelops you. The only sources of light come from the distant buildings and a few scattered street lamps that cast a soft glow on the pavement. You walk with purpose, your feet brushing against the cool surface as you navigate through the quiet streets. The occasional car headlights streak by in the distance, their bright beams cutting through the shadows. The air is crisp and cool, suggesting it might be late autumn or early winter. Your breath is visible in the cold night air, adding to the sense of movement and life amidst the stillness.\" \n",
      "\n",
      "Time is 560.7288293333332. Action narration is \"You enter the house.\".\n",
      "Detailed Description: \"With a sense of anticipation, you step into the house. The threshold beneath your feet creaks slightly as you transition from the outdoors to the indoor environment. The interior is warmly lit, casting a cozy glow on the tiled floor and the surrounding walls. To your right, a tall black pillar stands prominently against the backdrop of a dark wall with horizontal slats. Potted plants are neatly arranged along the base of this pillar, their green leaves contrasting with the muted tones of the architecture. As you move forward, the sound of footsteps echoes softly in the quiet space, adding to the atmosphere of entering a private sanctuary.\" \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(next(caption_merger2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clipuid2cliptime(origin_narrations, video_uid, clip_uid):\n",
    "    summs = origin_narrations[video_uid]['summaries']\n",
    "    is_match = False\n",
    "    for summ in summs:\n",
    "        if summ['_annotation_uid'] == clip_uid:\n",
    "            is_match = True\n",
    "            break\n",
    "    if not is_match:\n",
    "        return None, None\n",
    "    clip_start_time = summ['start_time']\n",
    "    clip_end_time = summ['end_time']\n",
    "    return clip_start_time, clip_end_time\n",
    "\n",
    "\n",
    "def parse_qa_file(file_path, video_uid, clip_uid):\n",
    "    \"\"\"\n",
    "    解析 .txt 文件，将每一组 QA 和任务类型转化为字典\n",
    "    \n",
    "    参数:\n",
    "    - file_path: .txt 文件路径\n",
    "    \n",
    "    返回:\n",
    "    - qa_list: 包含所有 QA 和任务类型的字典列表\n",
    "    \"\"\"\n",
    "    qa_list = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    step_idx = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'step' in line.lower():\n",
    "            step_idx = i\n",
    "    lines = lines[step_idx:]\n",
    "    \n",
    "    clip_start_time, clip_end_time = clipuid2cliptime(origin_narration, video_uid, clip_uid)\n",
    "    if clip_end_time is None:\n",
    "        return []\n",
    "    current_qa = {}\n",
    "    current_a = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        line = line.replace('*', '')\n",
    "        if \"Task Type:\" in line:\n",
    "            current_qa[\"Task Type\"] = line.split(\": \")[1]\n",
    "        elif \"Question:\" in line or \"Q:\" in line:\n",
    "            current_qa[\"question\"] = line.split(\": \")[1].strip()\n",
    "        elif \"Answer:\" in line or \"A:\" in line:\n",
    "            current_a.append(line.split(\":\")[1].strip())\n",
    "        elif \"Time:\" in line:\n",
    "            current_a.append(line.split(\":\")[1].strip())\n",
    "        elif line == \"\":\n",
    "            if current_qa:\n",
    "                conversation = []\n",
    "                for i in range(len(current_a)):\n",
    "                    if i % 2 == 0:\n",
    "                        beta = beta_map.get(clip_uid, 0)\n",
    "                        t = float(current_a[i+1])\n",
    "                        start_time = t - beta / (2 * alpha)\n",
    "                        end_time = t + beta / (2 * alpha)\n",
    "                        conversation.append(\n",
    "                            {\n",
    "                                'role': 'assistant',\n",
    "                                'content': current_a[i],\n",
    "                                'time': t,\n",
    "                                'start_time': start_time,\n",
    "                                'end_time': end_time,\n",
    "                            }\n",
    "                        )\n",
    "                current_qa[\"conversation\"] = conversation\n",
    "                qa_list.append(current_qa)\n",
    "                current_qa = {}\n",
    "                current_a = []\n",
    "    \n",
    "    # 添加最后一组 QA（如果文件末尾没有空行）\n",
    "    if current_qa:\n",
    "        conversation = []\n",
    "        for i in range(len(current_a)):\n",
    "            if i % 2 == 0:\n",
    "                beta = beta_map.get(clip_uid, 0)\n",
    "                try:\n",
    "                    t = float(current_a[i+1])\n",
    "                except:\n",
    "                    print(video_uid, clip_uid, current_a)\n",
    "                start_time = t - beta / (2 * alpha)\n",
    "                end_time = t + beta / (2 * alpha)\n",
    "                conversation.append(\n",
    "                    {\n",
    "                        'role': 'assistant',\n",
    "                        'content': current_a[i],\n",
    "                        'time': t,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                    }\n",
    "                )\n",
    "        current_qa[\"conversation\"] = conversation\n",
    "        qa_list.append(current_qa)\n",
    "        \n",
    "        \n",
    "    for qa in qa_list:\n",
    "        qa[\"clip_start_time\"]=clip_start_time\n",
    "        qa[\"clip_end_time\"]=clip_end_time\n",
    "    return qa_list\n",
    "\n",
    "output_dir = '/home/zhangyl/videollm-online/dataset/move_action_function_v8/'\n",
    "postfix = 'qa.txt'\n",
    "file_list = os.listdir(output_dir)\n",
    "annos = {}\n",
    "for file in file_list:\n",
    "    if file.endswith(postfix):\n",
    "        video_uid, clip_uid = file.split('_')[:2]\n",
    "        if video_uid not in annos:\n",
    "            annos[video_uid] = {}\n",
    "        annos[video_uid][clip_uid] = parse_qa_file(os.path.join(output_dir, file),\n",
    "                                                   video_uid, clip_uid)\n",
    "\n",
    "with open('/home/zhangyl/videollm-online/data/estp/annotations/move_action_function_v8.json', 'w') as f:\n",
    "    json.dump(annos,f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
